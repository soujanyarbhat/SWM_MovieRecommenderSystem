{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as weight_init\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"Model\"\n",
    "MODEL_VER = \"2\"\n",
    "MODEL_FILE = \"model.pt\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_VER)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "dropout = 0.1\n",
    "epochs = 200\n",
    "activation = \"sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reference: https://medium.com/@haoyunlai/pytorch-implementation-of-autoencoder-based-recommender-system-9aff6c3d1b02\"\"\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for Symmetrical AutoEncoder Network\n",
    "    :param L: List of int, contains sizes of encoding layers and starts with feature size\n",
    "    For example: [500, 20, 10] will result in:\n",
    "      - encoder 2 layers: 500x20 and 20x10. Representation layer (z) will be 10\n",
    "      - decoder 2 layers: 10x20 and 20x500, output size is 500, reconstructed.\n",
    "    :param activation_fn: (default 'sigmoid') Type of activation function\n",
    "    :param drop_prob: (default: 0.0) Dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, L, activation_fn='sigmoid', drop_prob=0.0):\n",
    "        super(Encoder, self).__init__()\n",
    "        layers = self.create_nn_structure(L)\n",
    "        self.num_layers = len(L)\n",
    "        # create controller for activation function\n",
    "        self.activation_fn_nm = activation_fn\n",
    "        # create dropout module\n",
    "        self._drop_prob = drop_prob\n",
    "        if drop_prob > 0.0:\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "        #initialize with empty list to store layers\n",
    "        self.linears = nn.ModuleList([])\n",
    "        self.linears.extend([nn.Linear(i[0], i[1]) for i in layers])\n",
    "        \n",
    "    def get_activation_fn(self):\n",
    "        # user selected activation function at layers except for last layer\n",
    "        if self.activation_fn_nm == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif self.activation_fn_nm == 'lrelu':\n",
    "            return nn.LeakyReLU()\n",
    "        elif self.activation_fn_nm == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError('Activation function type not defined')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i,layer in enumerate(self.linears):\n",
    "            if i <= self.num_layers-1:\n",
    "                # create instance of activation function\n",
    "                act_fn = self.get_activation_fn()\n",
    "                # pass in the input\n",
    "                x = act_fn(self.linears[i](x))\n",
    "                if self._drop_prob > 0.0 and i <= int(self.num_layers/2): \n",
    "                    # apply dropout only on encode layer by control of i\n",
    "                    x = self.dropout(x)\n",
    "        # No activation on the last decoding layer\n",
    "        x = self.linears[-1](x)\n",
    "        return x\n",
    "\n",
    "    def create_nn_structure(self, L):\n",
    "        max_ind = len(L)-1\n",
    "        layers = []\n",
    "        for i,v in enumerate(L):\n",
    "            if i < max_ind:\n",
    "                #still have i+1 available, create layer tuple\n",
    "                layer = [v,L[i+1]]\n",
    "                layers.append(layer)\n",
    "        #then inverse the layers for decoder size\n",
    "        encoder_layers = layers[:]\n",
    "        for l in encoder_layers[::-1]:\n",
    "            decoder_layer = l[::-1]\n",
    "            layers.append(decoder_layer)\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_mapping(L):\n",
    "    '''\n",
    "    return reindexed dict on user and items\n",
    "    encoded indices starts from 1\n",
    "    input: \n",
    "    * L: list of str\n",
    "    outputs:\n",
    "    * ind_2_item,item_2_ind: tuple of dictionary\n",
    "    '''\n",
    "    L = set(L)\n",
    "    ind_2_item = {}\n",
    "    \n",
    "    for i,v in enumerate(L):\n",
    "        #index start from 1\n",
    "        ind_2_item[i+1] = v\n",
    "    #invert the map\n",
    "    item_2_ind = {v: k for k, v in ind_2_item.items()}\n",
    "    return ind_2_item,item_2_ind\n",
    "\n",
    "def reindexer(ratings_df,user_col,item_col,rating_col):\n",
    "    '''\n",
    "    inputs:\n",
    "    * ratings_df: pandas df containing ratings/affinity for user-item pairs\n",
    "    * user_col: actual col name for users\n",
    "    * item_col: actual col name for items\n",
    "    * rating_col: actual col name for ratings\n",
    "    output:\n",
    "    * ratings_df: reindexed user and item column, pandas df\n",
    "    '''\n",
    "    users_list = ratings_df[user_col].tolist()\n",
    "    item_list = ratings_df[item_col].tolist()\n",
    "    \n",
    "    ind_2_user,user_2_ind = create_index_mapping(users_list)\n",
    "    ind_2_item,item_2_ind = create_index_mapping(item_list)\n",
    "    \n",
    "    #rename ratings df\n",
    "    ratings_df = ratings_df.rename(columns={user_col:'user_col',\n",
    "                                            item_col:'item_col',\n",
    "                                            rating_col:'rating_col'})\n",
    "    \n",
    "    #encode df using the 2 mappings\n",
    "    ratings_df['encoded_users'] = ratings_df['user_col'].apply(lambda x:user_2_ind[x])\n",
    "    ratings_df['encoded_items'] = ratings_df['item_col'].apply(lambda x:item_2_ind[x])\n",
    "    \n",
    "    return ratings_df[['encoded_users','encoded_items','rating_col']], ind_2_user,user_2_ind, ind_2_item,item_2_ind\n",
    "\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(nb_users+1):\n",
    "        # each user's watched movies\n",
    "        # data[:,0], first column, all rows column users\n",
    "        id_items = data[:,1][data[:,0] == id_users]\n",
    "        # each user's rating for that item\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        # the positions of these items are filled with ratings, creating the matrix\n",
    "        ratings[id_items-1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_movies(movies, item_2_ind):\n",
    "    movies['movieid'] = movies['movieid'].apply(lambda x:item_2_ind[x] if x in item_2_ind else -1)\n",
    "\n",
    "def extract_genre_values(movies):\n",
    "    genre = movies['genre']\n",
    "    unique_genre = genre.unique()\n",
    "    genre_values = []\n",
    "    for movie_genre in unique_genre:\n",
    "        mg = movie_genre.split(\"|\")\n",
    "        for g in mg:\n",
    "            if g not in genre_values:\n",
    "                genre_values.append(g)\n",
    "\n",
    "    return sorted(genre_values, key=str.lower)\n",
    "\n",
    "# get genre vector\n",
    "def get_genre_vector(genre_row_val):\n",
    "    mg = genre_row_val.split(\"|\")\n",
    "    gen_vec = np.zeros(len(genre_values))\n",
    "    gen_index = 0\n",
    "    for g in genre_values:\n",
    "        if g in mg:\n",
    "            gen_vec[gen_index] = 1\n",
    "        gen_index += 1\n",
    "    return gen_vec\n",
    "\n",
    "# Add Genre Vector to movies dataframe\n",
    "def add_movies_genre(movies):\n",
    "#     movie_col = []\n",
    "\n",
    "#     movie_2_genre = {}\n",
    "#     for row in movies.iterrows():\n",
    "#         print(row)\n",
    "#         gen_vec = get_genre_vector(row['genre'])\n",
    "#         movie_col.append(gen_vec)\n",
    "#         movie_2_genre[row['movieid']] = np.array(gen_vec)\n",
    "\n",
    "    movies['genre_vector'] = movies['genre'].apply(lambda x:np.array(get_genre_vector(x)))\n",
    "    \n",
    "    movie_2_genre = pd.Series(movies.genre_vector.values,index = movies.movieid).to_dict()\n",
    "\n",
    "    return movie_2_genre\n",
    "\n",
    "\n",
    "# def addgenrevector(data, movie_2_genre):\n",
    "#     genre_array = []\n",
    "#     movie_id_list = data['encoded_items'].tolist()\n",
    "#     for movie_id in movie_id_list:\n",
    "#         genre_array.append(movie_2_genre[movie_id])\n",
    "#     data['genre_vector'] = genre_array\n",
    "    \n",
    "def get_user_genre(movies, ratings):\n",
    "    genres = np.zeros(nb_genres)\n",
    "    for movie, rating in zip(movies, ratings):\n",
    "        if rating>3:\n",
    "            genres += movie_2_genre[movie]\n",
    "    return genres\n",
    "    \n",
    "    \n",
    "def convert_with_side_info(data):\n",
    "    new_data = []\n",
    "    for id_users in range(nb_users+1):\n",
    "        # each user's watched movies\n",
    "        # data[:,0], first column, all rows column users\n",
    "        id_items = data[:,1][data[:,0] == id_users]\n",
    "        # each user's rating for that item\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        \n",
    "        ratings = np.zeros(nb_movies)\n",
    "        \n",
    "        genres = get_user_genre(id_items, id_ratings)\n",
    "                \n",
    "        # the positions of these items are filled with ratings, creating the matrix\n",
    "        ratings[id_items-1] = id_ratings\n",
    "        \n",
    "        new_data.append(list(np.append(ratings,genres)))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('MovieLens1M/ratings.dat', delimiter = '::',header=None, engine='python')\n",
    "movies = pd.read_csv('MovieLens1M/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "users = pd.read_csv('MovieLens1M/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "\n",
    "ratings.columns = ['userid','movieid','rating','timestamp']\n",
    "ratings_reindex, ind_2_user,user_2_ind , ind_2_item,item_2_ind = reindexer(ratings,'userid','movieid','rating')\n",
    "\n",
    "# Adding Side Information\n",
    "movies.columns = ['movieid', 'movie', 'genre']\n",
    "users.columns = ['userid', 'gender', 'age', 'occupation', 'zipcode']\n",
    "\n",
    "users['female_user'] = (users['gender'] == 'F').astype(int)\n",
    "users['male_user'] = (users['gender'] == 'M').astype(int)\n",
    "\n",
    "reindex_movies(movies, item_2_ind)\n",
    "genre_values = extract_genre_values(movies)\n",
    "movie_2_genre = add_movies_genre(movies)\n",
    "\n",
    "train, test = train_test_split(ratings_reindex,\n",
    "                               stratify=ratings_reindex['encoded_users'],\n",
    "                               test_size=0.1,\n",
    "                               random_state=42)\n",
    "\n",
    "training_set = np.array(train, dtype='int')\n",
    "test_set = np.array(test, dtype='int')\n",
    "\n",
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))\n",
    "nb_genres = len(genre_values)\n",
    "  \n",
    "training_set = convert_with_side_info(training_set)\n",
    "test_set = convert_with_side_info(test_set)\n",
    "\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 2., 0.],\n",
      "        [0., 0., 0.,  ..., 3., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train loss: 1.2772270441055298|| test loss: 0.9871353507041931\n",
      "epoch: 2 train loss: 0.9965866208076477|| test loss: 0.9620037078857422\n",
      "epoch: 3 train loss: 0.9830197095870972|| test loss: 0.9564685225486755\n",
      "epoch: 4 train loss: 0.9778309464454651|| test loss: 0.9532893896102905\n",
      "epoch: 5 train loss: 0.9760851263999939|| test loss: 0.9529081583023071\n",
      "epoch: 6 train loss: 0.9742476344108582|| test loss: 0.9513510465621948\n",
      "epoch: 7 train loss: 0.9733386039733887|| test loss: 0.9491996765136719\n",
      "epoch: 8 train loss: 0.9727285504341125|| test loss: 0.9500186443328857\n",
      "epoch: 9 train loss: 0.9729252457618713|| test loss: 0.9515034556388855\n",
      "epoch: 10 train loss: 0.9723675847053528|| test loss: 0.9493308663368225\n",
      "epoch: 11 train loss: 0.9718551635742188|| test loss: 0.9492770433425903\n",
      "epoch: 12 train loss: 0.9715361595153809|| test loss: 0.9488109946250916\n",
      "epoch: 13 train loss: 0.9718716144561768|| test loss: 0.9506832957267761\n",
      "epoch: 14 train loss: 0.9717561602592468|| test loss: 0.9494384527206421\n",
      "epoch: 15 train loss: 0.972226083278656|| test loss: 0.951065719127655\n",
      "epoch: 16 train loss: 0.9716189503669739|| test loss: 0.9491652846336365\n",
      "epoch: 17 train loss: 0.9717966914176941|| test loss: 0.9500954747200012\n",
      "epoch: 18 train loss: 0.9712308645248413|| test loss: 0.949381411075592\n",
      "epoch: 19 train loss: 0.9715291261672974|| test loss: 0.9508874416351318\n",
      "epoch: 20 train loss: 0.9714240431785583|| test loss: 0.9486276507377625\n",
      "epoch: 21 train loss: 0.9716206789016724|| test loss: 0.9493225812911987\n",
      "epoch: 22 train loss: 0.9711159467697144|| test loss: 0.9489597678184509\n",
      "epoch: 23 train loss: 0.9713176488876343|| test loss: 0.9500629305839539\n",
      "epoch: 24 train loss: 0.9710242748260498|| test loss: 0.9496045708656311\n",
      "epoch: 25 train loss: 0.9708471298217773|| test loss: 0.9500023722648621\n",
      "epoch: 26 train loss: 0.9713499546051025|| test loss: 0.9496614336967468\n",
      "epoch: 27 train loss: 0.9706414937973022|| test loss: 0.950697660446167\n",
      "epoch: 28 train loss: 0.9703035354614258|| test loss: 0.9486996531486511\n",
      "epoch: 29 train loss: 0.970678448677063|| test loss: 0.948921799659729\n",
      "epoch: 30 train loss: 0.9702229499816895|| test loss: 0.9476953148841858\n",
      "epoch: 31 train loss: 0.9696959257125854|| test loss: 0.9495040774345398\n",
      "epoch: 32 train loss: 0.9691196084022522|| test loss: 0.9488692879676819\n",
      "epoch: 33 train loss: 0.9712681174278259|| test loss: 0.9493739008903503\n",
      "epoch: 34 train loss: 0.9711953401565552|| test loss: 0.949627697467804\n",
      "epoch: 35 train loss: 0.971333384513855|| test loss: 0.9500733017921448\n",
      "epoch: 36 train loss: 0.9708650708198547|| test loss: 0.9494682550430298\n",
      "epoch: 37 train loss: 0.9712548851966858|| test loss: 0.9494804739952087\n",
      "epoch: 38 train loss: 0.9708486795425415|| test loss: 0.949556827545166\n",
      "epoch: 39 train loss: 0.9712562561035156|| test loss: 0.9499430656433105\n",
      "epoch: 40 train loss: 0.9710410833358765|| test loss: 0.9488295912742615\n",
      "epoch: 41 train loss: 0.9711671471595764|| test loss: 0.9497326016426086\n",
      "epoch: 42 train loss: 0.9709882736206055|| test loss: 0.9490917325019836\n",
      "epoch: 43 train loss: 0.971150815486908|| test loss: 0.9494840502738953\n",
      "epoch: 44 train loss: 0.9708990454673767|| test loss: 0.9487860798835754\n",
      "epoch: 45 train loss: 0.9708571434020996|| test loss: 0.9493271708488464\n",
      "epoch: 46 train loss: 0.9707840085029602|| test loss: 0.9491496086120605\n",
      "epoch: 47 train loss: 0.9708098769187927|| test loss: 0.9490734338760376\n",
      "epoch: 48 train loss: 0.9706220030784607|| test loss: 0.9478562474250793\n",
      "epoch: 49 train loss: 0.969925045967102|| test loss: 0.946975827217102\n",
      "epoch: 50 train loss: 0.9691678285598755|| test loss: 0.94997239112854\n",
      "epoch: 51 train loss: 0.9688536524772644|| test loss: 0.9484763145446777\n",
      "epoch: 52 train loss: 0.9677897691726685|| test loss: 0.9469497203826904\n",
      "epoch: 53 train loss: 0.9683809280395508|| test loss: 0.9534488320350647\n",
      "epoch: 54 train loss: 0.9681466817855835|| test loss: 0.9458470344543457\n",
      "epoch: 55 train loss: 0.9667157530784607|| test loss: 0.9462143778800964\n",
      "epoch: 56 train loss: 0.9699676632881165|| test loss: 0.9503000378608704\n",
      "epoch: 57 train loss: 0.970687985420227|| test loss: 0.9489538073539734\n",
      "epoch: 58 train loss: 0.967626690864563|| test loss: 0.9455494284629822\n",
      "epoch: 59 train loss: 0.967176079750061|| test loss: 0.9448908567428589\n",
      "epoch: 60 train loss: 0.9681347012519836|| test loss: 0.9492629170417786\n",
      "epoch: 61 train loss: 0.9694366455078125|| test loss: 0.9487836956977844\n",
      "epoch: 62 train loss: 0.9677333235740662|| test loss: 0.9486479759216309\n",
      "epoch: 63 train loss: 0.9689704775810242|| test loss: 0.9481903314590454\n",
      "epoch: 64 train loss: 0.9668568968772888|| test loss: 0.9453077912330627\n",
      "epoch: 65 train loss: 0.9661752581596375|| test loss: 0.9444533586502075\n",
      "epoch: 66 train loss: 0.9643352627754211|| test loss: 0.9416847825050354\n",
      "epoch: 67 train loss: 0.9697911739349365|| test loss: 0.9486937522888184\n",
      "epoch: 68 train loss: 0.9692205786705017|| test loss: 0.9487106800079346\n",
      "epoch: 69 train loss: 0.9689556360244751|| test loss: 0.9480943083763123\n",
      "epoch: 70 train loss: 0.9686184525489807|| test loss: 0.9479029178619385\n",
      "epoch: 71 train loss: 0.9687549471855164|| test loss: 0.9478222131729126\n",
      "epoch: 72 train loss: 0.9681984782218933|| test loss: 0.9476243257522583\n",
      "epoch: 73 train loss: 0.9683015942573547|| test loss: 0.9485397934913635\n",
      "epoch: 74 train loss: 0.9680163264274597|| test loss: 0.9479822516441345\n",
      "epoch: 75 train loss: 0.9680607914924622|| test loss: 0.9472546577453613\n",
      "epoch: 76 train loss: 0.9661674499511719|| test loss: 0.9434964060783386\n",
      "epoch: 77 train loss: 0.9668222069740295|| test loss: 0.9494512677192688\n",
      "epoch: 78 train loss: 0.968246579170227|| test loss: 0.947837769985199\n",
      "epoch: 79 train loss: 0.9674060344696045|| test loss: 0.9442691802978516\n",
      "epoch: 80 train loss: 0.9664110541343689|| test loss: 0.9463439583778381\n",
      "epoch: 81 train loss: 0.9660524725914001|| test loss: 0.9416049122810364\n",
      "epoch: 82 train loss: 0.9661015868186951|| test loss: 0.9437247514724731\n",
      "epoch: 83 train loss: 0.9664289951324463|| test loss: 0.9486442804336548\n",
      "epoch: 84 train loss: 0.9667559862136841|| test loss: 0.9469917416572571\n",
      "epoch: 85 train loss: 0.9682251811027527|| test loss: 0.9467450976371765\n",
      "epoch: 86 train loss: 0.9676856398582458|| test loss: 0.9465519785881042\n",
      "epoch: 87 train loss: 0.9679018259048462|| test loss: 0.9468285441398621\n",
      "epoch: 88 train loss: 0.9676394462585449|| test loss: 0.9465097784996033\n",
      "epoch: 89 train loss: 0.9677555561065674|| test loss: 0.9467528462409973\n",
      "epoch: 90 train loss: 0.9674853682518005|| test loss: 0.9463289380073547\n",
      "epoch: 91 train loss: 0.9675434231758118|| test loss: 0.9466432332992554\n",
      "epoch: 92 train loss: 0.9674136638641357|| test loss: 0.9468657970428467\n",
      "epoch: 93 train loss: 0.9674395322799683|| test loss: 0.9465107917785645\n",
      "epoch: 94 train loss: 0.9672044515609741|| test loss: 0.9469347596168518\n",
      "epoch: 95 train loss: 0.9675195813179016|| test loss: 0.9470639824867249\n",
      "epoch: 96 train loss: 0.9671093821525574|| test loss: 0.9466925859451294\n",
      "epoch: 97 train loss: 0.9672347903251648|| test loss: 0.9471719861030579\n",
      "epoch: 98 train loss: 0.9669927954673767|| test loss: 0.9461960792541504\n",
      "epoch: 99 train loss: 0.9669926166534424|| test loss: 0.9461917281150818\n",
      "epoch: 100 train loss: 0.966381311416626|| test loss: 0.9461758136749268\n",
      "epoch: 101 train loss: 0.9658852219581604|| test loss: 0.9439824223518372\n",
      "epoch: 102 train loss: 0.966698408126831|| test loss: 0.9470356106758118\n",
      "epoch: 103 train loss: 0.967191755771637|| test loss: 0.946668267250061\n",
      "epoch: 104 train loss: 0.9670429229736328|| test loss: 0.9463216662406921\n",
      "epoch: 105 train loss: 0.9670128226280212|| test loss: 0.9468095302581787\n",
      "epoch: 106 train loss: 0.9668499231338501|| test loss: 0.9464783668518066\n",
      "epoch: 107 train loss: 0.9669157862663269|| test loss: 0.9464035630226135\n",
      "epoch: 108 train loss: 0.9665426015853882|| test loss: 0.9456424713134766\n",
      "epoch: 109 train loss: 0.9668465256690979|| test loss: 0.9469555616378784\n",
      "epoch: 110 train loss: 0.9666860103607178|| test loss: 0.946494996547699\n",
      "epoch: 111 train loss: 0.9669451713562012|| test loss: 0.9464705586433411\n",
      "epoch: 112 train loss: 0.966378390789032|| test loss: 0.9458964467048645\n",
      "epoch: 113 train loss: 0.9657540917396545|| test loss: 0.9430897831916809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 114 train loss: 0.9651744365692139|| test loss: 0.9439873099327087\n",
      "epoch: 115 train loss: 0.9640746712684631|| test loss: 0.9450961947441101\n",
      "epoch: 116 train loss: 0.9641889333724976|| test loss: 0.9433709383010864\n",
      "epoch: 117 train loss: 0.9651590585708618|| test loss: 0.9442311525344849\n",
      "epoch: 118 train loss: 0.9645746946334839|| test loss: 0.9436496496200562\n",
      "epoch: 119 train loss: 0.9653933048248291|| test loss: 0.9464319944381714\n",
      "epoch: 120 train loss: 0.966421365737915|| test loss: 0.9461266994476318\n",
      "epoch: 121 train loss: 0.9665314555168152|| test loss: 0.9466121792793274\n",
      "epoch: 122 train loss: 0.9664242267608643|| test loss: 0.9462850093841553\n",
      "epoch: 123 train loss: 0.9665544033050537|| test loss: 0.946110725402832\n",
      "epoch: 124 train loss: 0.9662100076675415|| test loss: 0.9462962746620178\n",
      "epoch: 125 train loss: 0.9664932489395142|| test loss: 0.94661545753479\n",
      "epoch: 126 train loss: 0.9662473201751709|| test loss: 0.9461702704429626\n",
      "epoch: 127 train loss: 0.9663709998130798|| test loss: 0.9463439583778381\n",
      "epoch: 128 train loss: 0.9662264585494995|| test loss: 0.9463818669319153\n",
      "epoch: 129 train loss: 0.9662874341011047|| test loss: 0.9461942315101624\n",
      "epoch: 130 train loss: 0.9659134745597839|| test loss: 0.9459196329116821\n",
      "epoch: 131 train loss: 0.9663147926330566|| test loss: 0.9462376236915588\n",
      "epoch: 132 train loss: 0.9660264253616333|| test loss: 0.9460656642913818\n",
      "epoch: 133 train loss: 0.9663212895393372|| test loss: 0.9459107518196106\n",
      "epoch: 134 train loss: 0.9658042192459106|| test loss: 0.9460060596466064\n",
      "epoch: 135 train loss: 0.9660531878471375|| test loss: 0.9462084770202637\n",
      "epoch: 136 train loss: 0.9659597873687744|| test loss: 0.9460300207138062\n",
      "epoch: 137 train loss: 0.9660886526107788|| test loss: 0.9461597800254822\n",
      "epoch: 138 train loss: 0.9657424688339233|| test loss: 0.9457961916923523\n",
      "epoch: 139 train loss: 0.9660062789916992|| test loss: 0.9461087584495544\n",
      "epoch: 140 train loss: 0.9657450914382935|| test loss: 0.9460955858230591\n",
      "epoch: 141 train loss: 0.9660529494285583|| test loss: 0.9462019205093384\n",
      "epoch: 142 train loss: 0.9656904339790344|| test loss: 0.9459337592124939\n",
      "epoch: 143 train loss: 0.9659625887870789|| test loss: 0.9460663199424744\n",
      "epoch: 144 train loss: 0.9657217264175415|| test loss: 0.9459731578826904\n",
      "epoch: 145 train loss: 0.9659280180931091|| test loss: 0.9461922645568848\n",
      "epoch: 146 train loss: 0.9655060172080994|| test loss: 0.9459612965583801\n",
      "epoch: 147 train loss: 0.9655435681343079|| test loss: 0.9458136558532715\n",
      "epoch: 148 train loss: 0.965679407119751|| test loss: 0.9459418058395386\n",
      "epoch: 149 train loss: 0.9658305644989014|| test loss: 0.9457430243492126\n",
      "epoch: 150 train loss: 0.9655592441558838|| test loss: 0.9455053210258484\n",
      "epoch: 151 train loss: 0.9658737778663635|| test loss: 0.945494532585144\n",
      "epoch: 152 train loss: 0.965427577495575|| test loss: 0.9429266452789307\n",
      "epoch: 153 train loss: 0.9651584625244141|| test loss: 0.9441483616828918\n",
      "epoch: 154 train loss: 0.9652937054634094|| test loss: 0.9439214468002319\n",
      "epoch: 155 train loss: 0.9641293287277222|| test loss: 0.9430168867111206\n",
      "epoch: 156 train loss: 0.9651724100112915|| test loss: 0.946011483669281\n",
      "epoch: 157 train loss: 0.9650905132293701|| test loss: 0.9457891583442688\n",
      "epoch: 158 train loss: 0.9653796553611755|| test loss: 0.9458797574043274\n",
      "epoch: 159 train loss: 0.9656513333320618|| test loss: 0.945285439491272\n",
      "epoch: 160 train loss: 0.9652784466743469|| test loss: 0.9456731081008911\n",
      "epoch: 161 train loss: 0.9656420946121216|| test loss: 0.945635199546814\n",
      "epoch: 162 train loss: 0.9652652740478516|| test loss: 0.9458143711090088\n",
      "epoch: 163 train loss: 0.9652895927429199|| test loss: 0.9460200667381287\n",
      "epoch: 164 train loss: 0.9648151397705078|| test loss: 0.9456064105033875\n",
      "epoch: 165 train loss: 0.9655948877334595|| test loss: 0.9457509517669678\n",
      "epoch: 166 train loss: 0.9651358723640442|| test loss: 0.946025013923645\n",
      "epoch: 167 train loss: 0.9655803442001343|| test loss: 0.9455963969230652\n",
      "epoch: 168 train loss: 0.965091347694397|| test loss: 0.9457381963729858\n",
      "epoch: 169 train loss: 0.9654386639595032|| test loss: 0.9458793997764587\n",
      "epoch: 170 train loss: 0.9650378227233887|| test loss: 0.9458568692207336\n",
      "epoch: 171 train loss: 0.965382993221283|| test loss: 0.9458467364311218\n",
      "epoch: 172 train loss: 0.9649907350540161|| test loss: 0.9457762837409973\n",
      "epoch: 173 train loss: 0.9653162360191345|| test loss: 0.9460708498954773\n",
      "epoch: 174 train loss: 0.9650231599807739|| test loss: 0.946050226688385\n",
      "epoch: 175 train loss: 0.9652401804924011|| test loss: 0.9457052946090698\n",
      "epoch: 176 train loss: 0.9648333191871643|| test loss: 0.9459659457206726\n",
      "epoch: 177 train loss: 0.965147852897644|| test loss: 0.9456936717033386\n",
      "epoch: 178 train loss: 0.964937150478363|| test loss: 0.9460191130638123\n",
      "epoch: 179 train loss: 0.9652632474899292|| test loss: 0.9456328749656677\n",
      "epoch: 180 train loss: 0.9649466872215271|| test loss: 0.9455395340919495\n",
      "epoch: 181 train loss: 0.964817225933075|| test loss: 0.9448919892311096\n",
      "epoch: 182 train loss: 0.9638509154319763|| test loss: 0.9455116987228394\n",
      "epoch: 183 train loss: 0.964263916015625|| test loss: 0.9452605843544006\n",
      "epoch: 184 train loss: 0.9627218246459961|| test loss: 0.9457753300666809\n",
      "epoch: 185 train loss: 0.963523805141449|| test loss: 0.945403516292572\n",
      "epoch: 186 train loss: 0.9647745490074158|| test loss: 0.9460087418556213\n",
      "epoch: 187 train loss: 0.9648438096046448|| test loss: 0.9453507661819458\n",
      "epoch: 188 train loss: 0.9643876552581787|| test loss: 0.945472776889801\n",
      "epoch: 189 train loss: 0.9648986458778381|| test loss: 0.9454599618911743\n",
      "epoch: 190 train loss: 0.9631301760673523|| test loss: 0.942699670791626\n",
      "epoch: 191 train loss: 0.9626985788345337|| test loss: 0.9440786838531494\n",
      "epoch: 192 train loss: 0.962754487991333|| test loss: 0.9448121190071106\n",
      "epoch: 193 train loss: 0.9630051851272583|| test loss: 0.9435721039772034\n",
      "epoch: 194 train loss: 0.9627334475517273|| test loss: 0.9437968730926514\n",
      "epoch: 195 train loss: 0.9623644948005676|| test loss: 0.9435378909111023\n",
      "epoch: 196 train loss: 0.962173342704773|| test loss: 0.9432781934738159\n",
      "epoch: 197 train loss: 0.9634514451026917|| test loss: 0.9453962445259094\n",
      "epoch: 198 train loss: 0.9644198417663574|| test loss: 0.9454139471054077\n",
      "epoch: 199 train loss: 0.9647834897041321|| test loss: 0.9457973837852478\n",
      "epoch: 200 train loss: 0.9637107253074646|| test loss: 0.9460349082946777\n"
     ]
    }
   ],
   "source": [
    "autoencoder_network = Encoder([(nb_movies+nb_genres), 20,10], activation, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(autoencoder_network.parameters(), lr = lr, weight_decay = 0.5)\n",
    "\n",
    "validationLoss = float(\"inf\")\n",
    "nb_epoch = epochs\n",
    "mask = torch.Tensor([1]*nb_movies+[0]*nb_genres)\n",
    "for epoch in range(1, nb_epoch+1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        #Select only rating related columns to compute loss\n",
    "        target_ratings = target[:, :nb_movies]\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = autoencoder_network(input)\n",
    "            output_ratings = output[:, :nb_movies]\n",
    "            target.require_grad = False\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data*mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    test_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "        target_ratings = target[:, :nb_movies]\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            with torch.no_grad():\n",
    "                output = autoencoder_network(input)\n",
    "                output_ratings = output[:, :nb_movies]\n",
    "                target.require_grad = False\n",
    "                output[target == 0] = 0\n",
    "                output = output*mask\n",
    "                target = target*mask\n",
    "                loss = criterion(output, target)\n",
    "                mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "                test_loss += np.sqrt(loss.data*mean_corrector)\n",
    "                s += 1.\n",
    "    print(f\"epoch: {epoch} train loss: {train_loss/s}|| test loss: {test_loss/s}\")\n",
    "    \n",
    "    # Saving Model\n",
    "    if validationLoss > (test_loss/s):\n",
    "        validationLoss = (test_loss/s)\n",
    "        torch.save(autoencoder_network.state_dict(), os.path.join(MODEL_PATH,MODEL_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder_network = Encoder([nb_movies, 20,10], activation, dropout)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.RMSprop(autoencoder_network.parameters(), lr = lr, weight_decay = 0.5)\n",
    "\n",
    "# validationLoss = float(\"inf\")\n",
    "# nb_epoch = epochs\n",
    "\n",
    "# for epoch in range(1, nb_epoch + 1):\n",
    "#     train_loss = 0\n",
    "#     s = 0.\n",
    "#     # s is the number of users who rated at least 1 movies\n",
    "#     for id_user in range(nb_users):\n",
    "#         input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "#         target = input.clone()\n",
    "#         if torch.sum(target.data > 0) > 0:\n",
    "#             output = autoencoder_network(input)\n",
    "#             target.require_grad = False\n",
    "#             output[target == 0] = 0\n",
    "#             loss = criterion(output, target)\n",
    "#             mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) #making this anyway not equal to 0, as this will be a denominator\n",
    "#             #mean_corrector is the avg of the error, only considering the movies having ratings (non-zero ratings) for computing mean of error\n",
    "#             loss.backward() # decide the direction the increment of weights\n",
    "#             #this call will just computing all the gradients required\n",
    "#             train_loss += np.sqrt(loss.data*mean_corrector)\n",
    "#             s += 1.\n",
    "#             optimizer.step() # decide the amount to update the weights\n",
    "    \n",
    "#     # Validation\n",
    "#     test_loss = 0\n",
    "#     s = 0.\n",
    "\n",
    "#     res = []\n",
    "#     targets = []\n",
    "\n",
    "#     # averaged difference between real rating and predicted rating\n",
    "#     for id_user in range(nb_users):\n",
    "#         input = Variable(training_set[id_user]).unsqueeze(0) # should keep the training set\n",
    "#         target = Variable(test_set[id_user]).unsqueeze(0) # to predict the other movies user not seen yet\n",
    "\n",
    "#         if torch.sum(target.data > 0) > 0:\n",
    "#             # make predictions\n",
    "#             with torch.no_grad():\n",
    "#                 output = autoencoder_network(input)\n",
    "#                 targets.append(target.detach().numpy())\n",
    "#                 res.append(output.detach().numpy()) \n",
    "#                 target.require_grad = False\n",
    "#                 output[target == 0] = 0 # dont want to measure the loss on the movies didnt get the actual rating from user \n",
    "#                 # force to 0 and difference / loss will be 0 for those entries\n",
    "#                 loss = criterion(output, target)\n",
    "\n",
    "#                 mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) \n",
    "#                 # only consider the movies that are rated in the test set, to be included in the loss\n",
    "#                 test_loss += np.sqrt(loss.data*mean_corrector)\n",
    "#                 s += 1.\n",
    "#     print(f\"epoch: {epoch} train loss: {train_loss/s}|| test loss: {test_loss/s}\")\n",
    "    \n",
    "#     # Saving Model\n",
    "#     if validationLoss > (test_loss/s):\n",
    "#         validationLoss = (test_loss/s)\n",
    "#         torch.save(autoencoder_network.state_dict(), os.path.join(MODEL_PATH,MODEL_FILE))\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9416)\n"
     ]
    }
   ],
   "source": [
    "print(validationLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making top k recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_top_k_recommendations(encoder,evidence,k,filter_seen=True):\n",
    "    '''\n",
    "    :param encoder: autoencoder instance\n",
    "    :param evidence: full set of seen ratings from all users\n",
    "    :param k: top k items (by output score)\n",
    "    :param filter_seen: (default True) filter controller to remove seen items from top k list\n",
    "    '''     \n",
    "    res = []\n",
    "    nb_users = evidence.shape[0]\n",
    "    # to find top scored items for each user\n",
    "    for id_user in range(nb_users):\n",
    "        encoder_input = Variable(evidence[id_user]).unsqueeze(0) # should keep the training set \n",
    "        encoder_output = encoder(encoder_input)\n",
    "        \n",
    "        target = Variable(evidence[id_user]).unsqueeze(0) # mask to find items not seen yet\n",
    "        if filter_seen:\n",
    "            encoder_output[target != 0] = 0 # force seen items scores to 0, will never get recommended\n",
    "        res.append(encoder_output.detach().numpy())\n",
    "        \n",
    "    res = [a[0] for a in res]\n",
    "    final_itemsets = []    \n",
    "    for each in res:\n",
    "        full_ratings_predicted = list(each)\n",
    "        full_ratings_indexed = list(enumerate(full_ratings_predicted))\n",
    "        final_itemsets.append(sorted(full_ratings_indexed,key=lambda x:x[1],reverse =True)[:k])\n",
    "        \n",
    "    return final_itemsets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWM",
   "language": "python",
   "name": "swm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
