{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWcyyxUDh7SGFYjSm/aIkj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soujanyarbhat/SWM_MovieRecommenderSystem/blob/main/SAE_ratings_genres.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryfyxT8aILAy"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.init as weight_init\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bkNEc_uIPEh"
      },
      "source": [
        "\n",
        "MODEL_DIR = \"Model\"\n",
        "MODEL_VER = \"2\"\n",
        "MODEL_FILE = \"model.pt\"\n",
        "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_VER)\n",
        "\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    os.makedirs(MODEL_PATH)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKgekqtqIQoX"
      },
      "source": [
        "lr = 0.01\n",
        "dropout = 0.1\n",
        "epochs = 200\n",
        "activation = \"sigmoid\"\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gvkIj-aIS0Y"
      },
      "source": [
        "def create_index_mapping(L):\n",
        "    '''\n",
        "    return reindexed dict on user and items\n",
        "    encoded indices starts from 1\n",
        "    input: \n",
        "    * L: list of str\n",
        "    outputs:\n",
        "    * ind_2_item,item_2_ind: tuple of dictionary\n",
        "    '''\n",
        "    L = set(L)\n",
        "    ind_2_item = {}\n",
        "    \n",
        "    for i,v in enumerate(L):\n",
        "        #index start from 1\n",
        "        ind_2_item[i+1] = v\n",
        "    #invert the map\n",
        "    item_2_ind = {v: k for k, v in ind_2_item.items()}\n",
        "    return ind_2_item,item_2_ind\n",
        "\n",
        "def reindexer(ratings_df,user_col,item_col,rating_col):\n",
        "    '''\n",
        "    inputs:\n",
        "    * ratings_df: pandas df containing ratings/affinity for user-item pairs\n",
        "    * user_col: actual col name for users\n",
        "    * item_col: actual col name for items\n",
        "    * rating_col: actual col name for ratings\n",
        "    output:\n",
        "    * ratings_df: reindexed user and item column, pandas df\n",
        "    '''\n",
        "    users_list = ratings_df[user_col].tolist()\n",
        "    item_list = ratings_df[item_col].tolist()\n",
        "    \n",
        "    ind_2_user,user_2_ind = create_index_mapping(users_list)\n",
        "    ind_2_item,item_2_ind = create_index_mapping(item_list)\n",
        "    \n",
        "    #rename ratings df\n",
        "    ratings_df = ratings_df.rename(columns={user_col:'user_col',\n",
        "                                            item_col:'item_col',\n",
        "                                            rating_col:'rating_col'})\n",
        "    \n",
        "    #encode df using the 2 mappings\n",
        "    ratings_df['encoded_users'] = ratings_df['user_col'].apply(lambda x:user_2_ind[x])\n",
        "    ratings_df['encoded_items'] = ratings_df['item_col'].apply(lambda x:item_2_ind[x])\n",
        "    \n",
        "    return ratings_df[['encoded_users','encoded_items','rating_col']], ind_2_user,user_2_ind, ind_2_item,item_2_ind\n",
        "\n",
        "def convert(data):\n",
        "    new_data = []\n",
        "    for id_users in range(nb_users+1):\n",
        "        # each user's watched movies\n",
        "        # data[:,0], first column, all rows column users\n",
        "        id_items = data[:,1][data[:,0] == id_users]\n",
        "        # each user's rating for that item\n",
        "        id_ratings = data[:,2][data[:,0] == id_users]\n",
        "        ratings = np.zeros(nb_movies)\n",
        "        # the positions of these items are filled with ratings, creating the matrix\n",
        "        ratings[id_items-1] = id_ratings\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2eMWF8oIuBC"
      },
      "source": [
        "def reindex_movies(movies, item_2_ind):\n",
        "    movies['movieid'] = movies['movieid'].apply(lambda x:item_2_ind[x] if x in item_2_ind else -1)\n",
        "\n",
        "def extract_genre_values(movies):\n",
        "    genre = movies['genre']\n",
        "    unique_genre = genre.unique()\n",
        "    genre_values = []\n",
        "    for movie_genre in unique_genre:\n",
        "        mg = movie_genre.split(\"|\")\n",
        "        for g in mg:\n",
        "            if g not in genre_values:\n",
        "                genre_values.append(g)\n",
        "\n",
        "    return sorted(genre_values, key=str.lower)\n",
        "\n",
        "# get genre vector\n",
        "def get_genre_vector(genre_row_val):\n",
        "    mg = genre_row_val.split(\"|\")\n",
        "    gen_vec = np.zeros(len(genre_values))\n",
        "    gen_index = 0\n",
        "    for g in genre_values:\n",
        "        if g in mg:\n",
        "            gen_vec[gen_index] = 1\n",
        "        gen_index += 1\n",
        "    return gen_vec\n",
        "\n",
        "# Add Genre Vector to movies dataframe\n",
        "def add_movies_genre(movies):\n",
        "#     movie_col = []\n",
        "\n",
        "#     movie_2_genre = {}\n",
        "#     for row in movies.iterrows():\n",
        "#         print(row)\n",
        "#         gen_vec = get_genre_vector(row['genre'])\n",
        "#         movie_col.append(gen_vec)\n",
        "#         movie_2_genre[row['movieid']] = np.array(gen_vec)\n",
        "\n",
        "    movies['genre_vector'] = movies['genre'].apply(lambda x:np.array(get_genre_vector(x)))\n",
        "    \n",
        "    movie_2_genre = pd.Series(movies.genre_vector.values,index = movies.movieid).to_dict()\n",
        "\n",
        "    return movie_2_genre\n",
        "\n",
        "\n",
        "# def addgenrevector(data, movie_2_genre):\n",
        "#     genre_array = []\n",
        "#     movie_id_list = data['encoded_items'].tolist()\n",
        "#     for movie_id in movie_id_list:\n",
        "#         genre_array.append(movie_2_genre[movie_id])\n",
        "#     data['genre_vector'] = genre_array\n",
        "    \n",
        "def get_user_genre(movies, ratings):\n",
        "    genres = np.zeros(nb_genres)\n",
        "    for movie, rating in zip(movies, ratings):\n",
        "        if rating>3:\n",
        "            genres += movie_2_genre[movie]\n",
        "    return genres\n",
        "    \n",
        "    \n",
        "def convert_with_side_info(data):\n",
        "    new_data = []\n",
        "    for id_users in range(nb_users+1):\n",
        "        # each user's watched movies\n",
        "        # data[:,0], first column, all rows column users\n",
        "        id_items = data[:,1][data[:,0] == id_users]\n",
        "        # each user's rating for that item\n",
        "        id_ratings = data[:,2][data[:,0] == id_users]\n",
        "        \n",
        "        ratings = np.zeros(nb_movies)\n",
        "        \n",
        "        genres = get_user_genre(id_items, id_ratings)\n",
        "                \n",
        "        # the positions of these items are filled with ratings, creating the matrix\n",
        "        ratings[id_items-1] = id_ratings\n",
        "        \n",
        "        new_data.append(list(np.append(ratings,genres)))\n",
        "    return new_data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnA45esLI3tn"
      },
      "source": [
        "ratings = pd.read_csv('/content/ratings.dat', delimiter = '::',header=None, engine='python')\n",
        "movies = pd.read_csv('/content/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv('/content/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "\n",
        "ratings.columns = ['userid','movieid','rating','timestamp']\n",
        "ratings_reindex, ind_2_user,user_2_ind , ind_2_item,item_2_ind = reindexer(ratings,'userid','movieid','rating')\n",
        "\n",
        "# Adding Side Information\n",
        "movies.columns = ['movieid', 'movie', 'genre']\n",
        "users.columns = ['userid', 'gender', 'age', 'occupation', 'zipcode']\n",
        "\n",
        "users['female_user'] = (users['gender'] == 'F').astype(int)\n",
        "users['male_user'] = (users['gender'] == 'M').astype(int)\n",
        "\n",
        "reindex_movies(movies, item_2_ind)\n",
        "genre_values = extract_genre_values(movies)\n",
        "movie_2_genre = add_movies_genre(movies)\n",
        "\n",
        "train, test = train_test_split(ratings_reindex,\n",
        "                               stratify=ratings_reindex['encoded_users'],\n",
        "                               test_size=0.1,\n",
        "                               random_state=42)\n",
        "\n",
        "training_set = np.array(train, dtype='int')\n",
        "test_set = np.array(test, dtype='int')\n",
        "\n",
        "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
        "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))\n",
        "nb_genres = len(genre_values)\n",
        "\n",
        "training_set = convert_with_side_info(training_set)\n",
        "test_set = convert_with_side_info(test_set)\n",
        "\n",
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-Ka-5cxJRJq"
      },
      "source": [
        "input_columns = nb_movies + nb_genres\n",
        "\n",
        "class SAE(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(SAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_columns, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.fc3 = nn.Linear(10, 20)\n",
        "        self.fc4 = nn.Linear(20, input_columns)\n",
        "        self.activation = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckJHmqRqOLDF",
        "outputId": "39257cf1-40cd-477b-f88e-27f22290c320"
      },
      "source": [
        "nb_epoch = 200\n",
        "for epoch in range(1, nb_epoch+1):\n",
        "    train_loss = 0\n",
        "    s = 0.\n",
        "    for id_user in range(nb_users):\n",
        "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "        target = input.clone()\n",
        "        #Select only rating related columns to compute loss\n",
        "        target_ratings = target[:, :nb_movies]\n",
        "        if torch.sum(target.data > 0) > 0:\n",
        "            output = sae(input)\n",
        "            output_ratings = output[:, :nb_movies]\n",
        "            target.require_grad = False\n",
        "            output[target == 0] = 0\n",
        "            loss = criterion(output_ratings, target_ratings)\n",
        "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "            loss.backward()\n",
        "            train_loss += np.sqrt(loss.data*mean_corrector)\n",
        "            s += 1.\n",
        "            optimizer.step()\n",
        "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))\n",
        "    \n",
        "    \n",
        "# Testing the SAE\n",
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
        "    target_ratings = target[:, :nb_movies]\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "        output = sae(input)\n",
        "        output_ratings = output[:, :nb_movies]\n",
        "        target.require_grad = False\n",
        "        output[target == 0] = 0\n",
        "        loss = criterion(output_ratings, target_ratings)\n",
        "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "        test_loss += np.sqrt(loss.data*mean_corrector)\n",
        "        s += 1.\n",
        "print('test loss: '+str(test_loss/s))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 loss: tensor(1.3888)\n",
            "epoch: 2 loss: tensor(0.9542)\n",
            "epoch: 3 loss: tensor(0.9209)\n",
            "epoch: 4 loss: tensor(0.9093)\n",
            "epoch: 5 loss: tensor(0.9036)\n",
            "epoch: 6 loss: tensor(0.9004)\n",
            "epoch: 7 loss: tensor(0.8985)\n",
            "epoch: 8 loss: tensor(0.8970)\n",
            "epoch: 9 loss: tensor(0.8949)\n",
            "epoch: 10 loss: tensor(0.8942)\n",
            "epoch: 11 loss: tensor(0.8933)\n",
            "epoch: 12 loss: tensor(0.8927)\n",
            "epoch: 13 loss: tensor(0.8927)\n",
            "epoch: 14 loss: tensor(0.8930)\n",
            "epoch: 15 loss: tensor(0.8909)\n",
            "epoch: 16 loss: tensor(0.8903)\n",
            "epoch: 17 loss: tensor(0.8892)\n",
            "epoch: 18 loss: tensor(0.8911)\n",
            "epoch: 19 loss: tensor(0.8896)\n",
            "epoch: 20 loss: tensor(0.8884)\n",
            "epoch: 21 loss: tensor(0.8885)\n",
            "epoch: 22 loss: tensor(0.8894)\n",
            "epoch: 23 loss: tensor(0.8873)\n",
            "epoch: 24 loss: tensor(0.8876)\n",
            "epoch: 25 loss: tensor(0.8886)\n",
            "epoch: 26 loss: tensor(0.8895)\n",
            "epoch: 27 loss: tensor(0.8886)\n",
            "epoch: 28 loss: tensor(0.8885)\n",
            "epoch: 29 loss: tensor(0.8861)\n",
            "epoch: 30 loss: tensor(0.8822)\n",
            "epoch: 31 loss: tensor(0.8837)\n",
            "epoch: 32 loss: tensor(0.8861)\n",
            "epoch: 33 loss: tensor(0.8833)\n",
            "epoch: 34 loss: tensor(0.8842)\n",
            "epoch: 35 loss: tensor(0.8900)\n",
            "epoch: 36 loss: tensor(0.8907)\n",
            "epoch: 37 loss: tensor(0.8875)\n",
            "epoch: 38 loss: tensor(0.8858)\n",
            "epoch: 39 loss: tensor(0.8852)\n",
            "epoch: 40 loss: tensor(0.8833)\n",
            "epoch: 41 loss: tensor(0.8828)\n",
            "epoch: 42 loss: tensor(0.8895)\n",
            "epoch: 43 loss: tensor(0.8886)\n",
            "epoch: 44 loss: tensor(0.8854)\n",
            "epoch: 45 loss: tensor(0.8848)\n",
            "epoch: 46 loss: tensor(0.8842)\n",
            "epoch: 47 loss: tensor(0.8850)\n",
            "epoch: 48 loss: tensor(0.8853)\n",
            "epoch: 49 loss: tensor(0.8829)\n",
            "epoch: 50 loss: tensor(0.8830)\n",
            "epoch: 51 loss: tensor(0.8794)\n",
            "epoch: 52 loss: tensor(0.8846)\n",
            "epoch: 53 loss: tensor(0.8784)\n",
            "epoch: 54 loss: tensor(0.8869)\n",
            "epoch: 55 loss: tensor(0.8892)\n",
            "epoch: 56 loss: tensor(0.8900)\n",
            "epoch: 57 loss: tensor(0.8904)\n",
            "epoch: 58 loss: tensor(0.8854)\n",
            "epoch: 59 loss: tensor(0.8834)\n",
            "epoch: 60 loss: tensor(0.8903)\n",
            "epoch: 61 loss: tensor(0.8904)\n",
            "epoch: 62 loss: tensor(0.8905)\n",
            "epoch: 63 loss: tensor(0.8904)\n",
            "epoch: 64 loss: tensor(0.8901)\n",
            "epoch: 65 loss: tensor(0.8902)\n",
            "epoch: 66 loss: tensor(0.8891)\n",
            "epoch: 67 loss: tensor(0.8877)\n",
            "epoch: 68 loss: tensor(0.8873)\n",
            "epoch: 69 loss: tensor(0.8888)\n",
            "epoch: 70 loss: tensor(0.8896)\n",
            "epoch: 71 loss: tensor(0.8895)\n",
            "epoch: 72 loss: tensor(0.8893)\n",
            "epoch: 73 loss: tensor(0.8887)\n",
            "epoch: 74 loss: tensor(0.8863)\n",
            "epoch: 75 loss: tensor(0.8888)\n",
            "epoch: 76 loss: tensor(0.8900)\n",
            "epoch: 77 loss: tensor(0.8897)\n",
            "epoch: 78 loss: tensor(0.8897)\n",
            "epoch: 79 loss: tensor(0.8895)\n",
            "epoch: 80 loss: tensor(0.8895)\n",
            "epoch: 81 loss: tensor(0.8896)\n",
            "epoch: 82 loss: tensor(0.8895)\n",
            "epoch: 83 loss: tensor(0.8894)\n",
            "epoch: 84 loss: tensor(0.8892)\n",
            "epoch: 85 loss: tensor(0.8885)\n",
            "epoch: 86 loss: tensor(0.8889)\n",
            "epoch: 87 loss: tensor(0.8878)\n",
            "epoch: 88 loss: tensor(0.8884)\n",
            "epoch: 89 loss: tensor(0.8892)\n",
            "epoch: 90 loss: tensor(0.8886)\n",
            "epoch: 91 loss: tensor(0.8874)\n",
            "epoch: 92 loss: tensor(0.8875)\n",
            "epoch: 93 loss: tensor(0.8884)\n",
            "epoch: 94 loss: tensor(0.8867)\n",
            "epoch: 95 loss: tensor(0.8863)\n",
            "epoch: 96 loss: tensor(0.8872)\n",
            "epoch: 97 loss: tensor(0.8875)\n",
            "epoch: 98 loss: tensor(0.8892)\n",
            "epoch: 99 loss: tensor(0.8891)\n",
            "epoch: 100 loss: tensor(0.8889)\n",
            "epoch: 101 loss: tensor(0.8891)\n",
            "epoch: 102 loss: tensor(0.8890)\n",
            "epoch: 103 loss: tensor(0.8890)\n",
            "epoch: 104 loss: tensor(0.8889)\n",
            "epoch: 105 loss: tensor(0.8890)\n",
            "epoch: 106 loss: tensor(0.8889)\n",
            "epoch: 107 loss: tensor(0.8889)\n",
            "epoch: 108 loss: tensor(0.8889)\n",
            "epoch: 109 loss: tensor(0.8888)\n",
            "epoch: 110 loss: tensor(0.8889)\n",
            "epoch: 111 loss: tensor(0.8888)\n",
            "epoch: 112 loss: tensor(0.8887)\n",
            "epoch: 113 loss: tensor(0.8887)\n",
            "epoch: 114 loss: tensor(0.8886)\n",
            "epoch: 115 loss: tensor(0.8886)\n",
            "epoch: 116 loss: tensor(0.8886)\n",
            "epoch: 117 loss: tensor(0.8886)\n",
            "epoch: 118 loss: tensor(0.8885)\n",
            "epoch: 119 loss: tensor(0.8885)\n",
            "epoch: 120 loss: tensor(0.8885)\n",
            "epoch: 121 loss: tensor(0.8884)\n",
            "epoch: 122 loss: tensor(0.8884)\n",
            "epoch: 123 loss: tensor(0.8884)\n",
            "epoch: 124 loss: tensor(0.8884)\n",
            "epoch: 125 loss: tensor(0.8883)\n",
            "epoch: 126 loss: tensor(0.8884)\n",
            "epoch: 127 loss: tensor(0.8883)\n",
            "epoch: 128 loss: tensor(0.8883)\n",
            "epoch: 129 loss: tensor(0.8882)\n",
            "epoch: 130 loss: tensor(0.8882)\n",
            "epoch: 131 loss: tensor(0.8882)\n",
            "epoch: 132 loss: tensor(0.8883)\n",
            "epoch: 133 loss: tensor(0.8882)\n",
            "epoch: 134 loss: tensor(0.8882)\n",
            "epoch: 135 loss: tensor(0.8881)\n",
            "epoch: 136 loss: tensor(0.8881)\n",
            "epoch: 137 loss: tensor(0.8881)\n",
            "epoch: 138 loss: tensor(0.8882)\n",
            "epoch: 139 loss: tensor(0.8882)\n",
            "epoch: 140 loss: tensor(0.8881)\n",
            "epoch: 141 loss: tensor(0.8880)\n",
            "epoch: 142 loss: tensor(0.8880)\n",
            "epoch: 143 loss: tensor(0.8880)\n",
            "epoch: 144 loss: tensor(0.8879)\n",
            "epoch: 145 loss: tensor(0.8880)\n",
            "epoch: 146 loss: tensor(0.8880)\n",
            "epoch: 147 loss: tensor(0.8879)\n",
            "epoch: 148 loss: tensor(0.8879)\n",
            "epoch: 149 loss: tensor(0.8879)\n",
            "epoch: 150 loss: tensor(0.8878)\n",
            "epoch: 151 loss: tensor(0.8879)\n",
            "epoch: 152 loss: tensor(0.8878)\n",
            "epoch: 153 loss: tensor(0.8878)\n",
            "epoch: 154 loss: tensor(0.8879)\n",
            "epoch: 155 loss: tensor(0.8878)\n",
            "epoch: 156 loss: tensor(0.8878)\n",
            "epoch: 157 loss: tensor(0.8877)\n",
            "epoch: 158 loss: tensor(0.8878)\n",
            "epoch: 159 loss: tensor(0.8877)\n",
            "epoch: 160 loss: tensor(0.8877)\n",
            "epoch: 161 loss: tensor(0.8877)\n",
            "epoch: 162 loss: tensor(0.8877)\n",
            "epoch: 163 loss: tensor(0.8875)\n",
            "epoch: 164 loss: tensor(0.8875)\n",
            "epoch: 165 loss: tensor(0.8873)\n",
            "epoch: 166 loss: tensor(0.8867)\n",
            "epoch: 167 loss: tensor(0.8875)\n",
            "epoch: 168 loss: tensor(0.8874)\n",
            "epoch: 169 loss: tensor(0.8873)\n",
            "epoch: 170 loss: tensor(0.8870)\n",
            "epoch: 171 loss: tensor(0.8856)\n",
            "epoch: 172 loss: tensor(0.8866)\n",
            "epoch: 173 loss: tensor(0.8852)\n",
            "epoch: 174 loss: tensor(0.8857)\n",
            "epoch: 175 loss: tensor(0.8873)\n",
            "epoch: 176 loss: tensor(0.8870)\n",
            "epoch: 177 loss: tensor(0.8870)\n",
            "epoch: 178 loss: tensor(0.8868)\n",
            "epoch: 179 loss: tensor(0.8868)\n",
            "epoch: 180 loss: tensor(0.8867)\n",
            "epoch: 181 loss: tensor(0.8865)\n",
            "epoch: 182 loss: tensor(0.8865)\n",
            "epoch: 183 loss: tensor(0.8865)\n",
            "epoch: 184 loss: tensor(0.8864)\n",
            "epoch: 185 loss: tensor(0.8863)\n",
            "epoch: 186 loss: tensor(0.8863)\n",
            "epoch: 187 loss: tensor(0.8863)\n",
            "epoch: 188 loss: tensor(0.8862)\n",
            "epoch: 189 loss: tensor(0.8858)\n",
            "epoch: 190 loss: tensor(0.8840)\n",
            "epoch: 191 loss: tensor(0.8833)\n",
            "epoch: 192 loss: tensor(0.8852)\n",
            "epoch: 193 loss: tensor(0.8832)\n",
            "epoch: 194 loss: tensor(0.8809)\n",
            "epoch: 195 loss: tensor(0.8825)\n",
            "epoch: 196 loss: tensor(0.8849)\n",
            "epoch: 197 loss: tensor(0.8858)\n",
            "epoch: 198 loss: tensor(0.8864)\n",
            "epoch: 199 loss: tensor(0.8862)\n",
            "epoch: 200 loss: tensor(0.8855)\n",
            "test loss: tensor(0.7387)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58TIDZtzVCGX",
        "outputId": "f3506d8e-04ea-4856-953e-795fabc33548"
      },
      "source": [
        "    print(f\"epoch: {epoch} train loss: {train_loss/s}|| test loss: {test_loss/s}\")\n",
        "    \n",
        "    # Saving Model\n",
        "    if validationLoss > (test_loss/s):\n",
        "        validationLoss = (test_loss/s)\n",
        "        torch.save(sae.state_dict(), os.path.join(MODEL_PATH,MODEL_FILE))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 200 train loss: 0.8854984641075134|| test loss: 0.7386822700500488\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}