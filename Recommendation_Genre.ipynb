{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as weight_init\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"Model\"\n",
    "MODEL_VER = \"2\"\n",
    "MODEL_FILE = \"model.pt\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_VER)\n",
    "    \n",
    "OUTPUT_PATH = \"Output\"\n",
    "OUTPUT_FILE = f\"output{MODEL_VER}.csv\"\n",
    "OUTPUT_R_FILE = f\"output_rating{MODEL_VER}.csv\"\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "dropout = 0.1\n",
    "epochs = 200\n",
    "activation = \"sigmoid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference: https://medium.com/@haoyunlai/pytorch-implementation-of-autoencoder-based-recommender-system-9aff6c3d1b02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for Symmetrical AutoEncoder Network\n",
    "    :param L: List of int, contains sizes of encoding layers and starts with feature size\n",
    "    For example: [500, 20, 10] will result in:\n",
    "      - encoder 2 layers: 500x20 and 20x10. Representation layer (z) will be 10\n",
    "      - decoder 2 layers: 10x20 and 20x500, output size is 500, reconstructed.\n",
    "    :param activation_fn: (default 'sigmoid') Type of activation function\n",
    "    :param drop_prob: (default: 0.0) Dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, L, activation_fn='sigmoid', drop_prob=0.0):\n",
    "        super(Encoder, self).__init__()\n",
    "        layers = self.create_nn_structure(L)\n",
    "        self.num_layers = len(L)\n",
    "        # create controller for activation function\n",
    "        self.activation_fn_nm = activation_fn\n",
    "        # create dropout module\n",
    "        self._drop_prob = drop_prob\n",
    "        if drop_prob > 0.0:\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "        #initialize with empty list to store layers\n",
    "        self.linears = nn.ModuleList([])\n",
    "        self.linears.extend([nn.Linear(i[0], i[1]) for i in layers])\n",
    "        \n",
    "    def get_activation_fn(self):\n",
    "        # user selected activation function at layers except for last layer\n",
    "        if self.activation_fn_nm == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif self.activation_fn_nm == 'lrelu':\n",
    "            return nn.LeakyReLU()\n",
    "        elif self.activation_fn_nm == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError('Activation function type not defined')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i,layer in enumerate(self.linears):\n",
    "            if i <= self.num_layers-1:\n",
    "                # create instance of activation function\n",
    "                act_fn = self.get_activation_fn()\n",
    "                # pass in the input\n",
    "                x = act_fn(self.linears[i](x))\n",
    "                if self._drop_prob > 0.0 and i <= int(self.num_layers/2): \n",
    "                    # apply dropout only on encode layer by control of i\n",
    "                    x = self.dropout(x)\n",
    "        # No activation on the last decoding layer\n",
    "        x = self.linears[-1](x)\n",
    "        return x\n",
    "\n",
    "    def create_nn_structure(self, L):\n",
    "        max_ind = len(L)-1\n",
    "        layers = []\n",
    "        for i,v in enumerate(L):\n",
    "            if i < max_ind:\n",
    "                #still have i+1 available, create layer tuple\n",
    "                layer = [v,L[i+1]]\n",
    "                layers.append(layer)\n",
    "        #then inverse the layers for decoder size\n",
    "        encoder_layers = layers[:]\n",
    "        for l in encoder_layers[::-1]:\n",
    "            decoder_layer = l[::-1]\n",
    "            layers.append(decoder_layer)\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_mapping(L):\n",
    "    '''\n",
    "    return reindexed dict on user and items\n",
    "    encoded indices starts from 1\n",
    "    input: \n",
    "    * L: list of str\n",
    "    outputs:\n",
    "    * ind_2_item,item_2_ind: tuple of dictionary\n",
    "    '''\n",
    "    L = set(L)\n",
    "    ind_2_item = {}\n",
    "    \n",
    "    for i,v in enumerate(L):\n",
    "        #index start from 1\n",
    "        ind_2_item[i+1] = v\n",
    "    #invert the map\n",
    "    item_2_ind = {v: k for k, v in ind_2_item.items()}\n",
    "    return ind_2_item,item_2_ind\n",
    "\n",
    "def reindexer(ratings_df,user_col,item_col,rating_col):\n",
    "    '''\n",
    "    inputs:\n",
    "    * ratings_df: pandas df containing ratings/affinity for user-item pairs\n",
    "    * user_col: actual col name for users\n",
    "    * item_col: actual col name for items\n",
    "    * rating_col: actual col name for ratings\n",
    "    output:\n",
    "    * ratings_df: reindexed user and item column, pandas df\n",
    "    '''\n",
    "    users_list = ratings_df[user_col].tolist()\n",
    "    item_list = ratings_df[item_col].tolist()\n",
    "    \n",
    "    ind_2_user,user_2_ind = create_index_mapping(users_list)\n",
    "    ind_2_item,item_2_ind = create_index_mapping(item_list)\n",
    "    \n",
    "    #rename ratings df\n",
    "    ratings_df = ratings_df.rename(columns={user_col:'user_col',\n",
    "                                            item_col:'item_col',\n",
    "                                            rating_col:'rating_col'})\n",
    "    \n",
    "    #encode df using the 2 mappings\n",
    "    ratings_df['encoded_users'] = ratings_df['user_col'].apply(lambda x:user_2_ind[x])\n",
    "    ratings_df['encoded_items'] = ratings_df['item_col'].apply(lambda x:item_2_ind[x])\n",
    "    \n",
    "    return ratings_df[['encoded_users','encoded_items','rating_col']], ind_2_user,user_2_ind, ind_2_item,item_2_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_movies(movies, item_2_ind):\n",
    "    movies['movieid'] = movies['movieid'].apply(lambda x:item_2_ind[x] if x in item_2_ind else -1)\n",
    "    id_2_movie = pd.Series(movies.movie.values,index = movies.movieid).to_dict()\n",
    "\n",
    "    return id_2_movie\n",
    "\n",
    "def extract_genre_values(movies):\n",
    "    genre = movies['genre']\n",
    "    unique_genre = genre.unique()\n",
    "    genre_values = []\n",
    "    for movie_genre in unique_genre:\n",
    "        mg = movie_genre.split(\"|\")\n",
    "        for g in mg:\n",
    "            if g not in genre_values:\n",
    "                genre_values.append(g)\n",
    "\n",
    "    return sorted(genre_values, key=str.lower)\n",
    "\n",
    "# get genre vector\n",
    "def get_genre_vector(genre_row_val):\n",
    "    mg = genre_row_val.split(\"|\")\n",
    "    gen_vec = np.zeros(len(genre_values))\n",
    "    gen_index = 0\n",
    "    for g in genre_values:\n",
    "        if g in mg:\n",
    "            gen_vec[gen_index] = 1\n",
    "        gen_index += 1\n",
    "    return gen_vec\n",
    "\n",
    "# Add Genre Vector to movies dataframe\n",
    "def add_movies_genre(movies):\n",
    "    \n",
    "    movies['genre_vector'] = movies['genre'].apply(lambda x:np.array(get_genre_vector(x)))\n",
    "    \n",
    "    movie_2_genre = pd.Series(movies.genre_vector.values,index = movies.movieid).to_dict()\n",
    "\n",
    "    return movie_2_genre\n",
    "\n",
    "# Add genre preferences to users\n",
    "def get_user_genre(movies, ratings):\n",
    "    genres = np.zeros(nb_genres)\n",
    "    for movie, rating in zip(movies, ratings):\n",
    "        if rating>2:\n",
    "            genres += movie_2_genre[movie]\n",
    "    return genres\n",
    "    \n",
    "    \n",
    "def convert_with_side_info(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1,nb_users+1):\n",
    "        \n",
    "        id_items = data[:,1][data[:,0] == id_users]\n",
    "\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        \n",
    "        ratings = np.zeros(nb_movies)\n",
    "        \n",
    "        genres = get_user_genre(id_items, id_ratings)\n",
    "                \n",
    "        ratings[id_items-1] = id_ratings\n",
    "        \n",
    "        new_data.append(list(np.append(ratings,genres)))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('MovieLens1M/ratings.dat', delimiter = '::',header=None, engine='python')\n",
    "movies = pd.read_csv('MovieLens1M/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "users = pd.read_csv('MovieLens1M/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "\n",
    "ratings.columns = ['userid','movieid','rating','timestamp']\n",
    "ratings_reindex, ind_2_user,user_2_ind , ind_2_item,item_2_ind = reindexer(ratings,'userid','movieid','rating')\n",
    "\n",
    "# Adding Side Information\n",
    "movies.columns = ['movieid', 'movie', 'genre']\n",
    "users.columns = ['userid', 'gender', 'age', 'occupation', 'zipcode']\n",
    "\n",
    "users['female_user'] = (users['gender'] == 'F').astype(int)\n",
    "users['male_user'] = (users['gender'] == 'M').astype(int)\n",
    "\n",
    "id_2_movie = reindex_movies(movies, item_2_ind)\n",
    "genre_values = extract_genre_values(movies)\n",
    "movie_2_genre = add_movies_genre(movies)\n",
    "\n",
    "train, test = train_test_split(ratings_reindex,\n",
    "                               stratify=ratings_reindex['encoded_users'],\n",
    "                               test_size=0.1,\n",
    "                               random_state=42)\n",
    "\n",
    "training_set = np.array(train, dtype='int')\n",
    "test_set = np.array(test, dtype='int')\n",
    "\n",
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))\n",
    "nb_genres = len(genre_values)\n",
    "  \n",
    "training_set = convert_with_side_info(training_set)\n",
    "test_set = convert_with_side_info(test_set)\n",
    "\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)\n",
    "\n",
    "print(f\"Number of users: {nb_users}, Number of Movies: {nb_movies}, Number of Genres: {nb_genres}\")\n",
    "print(f\"Training set size: {training_set.shape}, Validation set size: {test_set.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_network = Encoder([(nb_movies+nb_genres), 20,10], activation, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(autoencoder_network.parameters(), lr = lr, weight_decay = 0.5)\n",
    "\n",
    "validationLoss = float(\"inf\")\n",
    "minEpoch = epochs\n",
    "nb_epoch = epochs\n",
    "\n",
    "for epoch in range(1, nb_epoch+1):\n",
    "    autoencoder_network.train()\n",
    "    train_loss = 0\n",
    "    train_s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input_ids = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        #Select only rating related columns to compute loss\n",
    "        target_ratings = target[:, :nb_movies]\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = autoencoder_network(input_ids)\n",
    "            \n",
    "            output[target==0] = 0\n",
    "            output_ratings = output[:, :nb_movies]\n",
    "            target.require_grad = False\n",
    "            \n",
    "            loss = criterion(output_ratings, target_ratings)\n",
    "            loss.backward()\n",
    "            \n",
    "            mean_corrector = nb_movies/float(torch.sum(target_ratings.data > 0) + 1e-10)\n",
    "            train_loss += np.sqrt(loss.data*mean_corrector)\n",
    "            train_s += 1.\n",
    "            optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Validation\n",
    "    autoencoder_network.eval()\n",
    "    test_loss = 0\n",
    "    test_s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input_ids = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "        target_ratings = target[:, :nb_movies]\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            with torch.no_grad():\n",
    "                output = autoencoder_network(input_ids)\n",
    "            \n",
    "                output[target==0] = 0\n",
    "                output_ratings = output[:, :nb_movies]\n",
    "                target.require_grad = False\n",
    "\n",
    "                loss = criterion(output_ratings, target_ratings)\n",
    "                mean_corrector = nb_movies/float(torch.sum(target_ratings.data > 0) + 1e-10)\n",
    "                \n",
    "                test_loss += np.sqrt(loss.data*mean_corrector)\n",
    "                test_s += 1.\n",
    "    print(f\"epoch: {epoch} train loss: {train_loss/train_s}|| test loss: {test_loss/test_s}\")\n",
    "    \n",
    "    # Saving Model\n",
    "    if validationLoss > (test_loss/test_s):\n",
    "        validationLoss = (test_loss/test_s)\n",
    "        minEpoch = epoch\n",
    "        torch.save(autoencoder_network.state_dict(), os.path.join(MODEL_PATH,MODEL_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE obtained: {validationLoss.data} after {minEpoch} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making top k recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "autoencoder_network = Encoder([(nb_movies+nb_genres), 20,10], activation, dropout)\n",
    "autoencoder_network.load_state_dict(torch.load(os.path.join(MODEL_PATH,MODEL_FILE)))\n",
    "autoencoder_network.eval()\n",
    "\n",
    "evidence = np.array(ratings_reindex, dtype='int')\n",
    "evidence = convert_with_side_info(evidence)\n",
    "evidence = torch.FloatTensor(evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_top_k_recommendations(encoder,evidence,k,filter_seen=True):\n",
    "    '''\n",
    "    :param encoder: autoencoder instance\n",
    "    :param evidence: full set of seen ratings from all users\n",
    "    :param k: top k items (by output score)\n",
    "    :param filter_seen: (default True) filter controller to remove seen items from top k list\n",
    "    '''     \n",
    "    res = []\n",
    "    nb_users = evidence.shape[0]\n",
    "    # to find top scored items for each user\n",
    "    for id_user in range(nb_users):\n",
    "        encoder_input = Variable(evidence[id_user]).unsqueeze(0) # should keep the training set \n",
    "        encoder_output = encoder(encoder_input)\n",
    "        \n",
    "        target = Variable(evidence[id_user]).unsqueeze(0) # mask to find items not seen yet\n",
    "        if filter_seen:\n",
    "            encoder_output[target != 0] = 0 # force seen items scores to 0, will never get recommended\n",
    "        res.append(encoder_output.detach().numpy())\n",
    "        \n",
    "    res = [a[0] for a in res]\n",
    "    final_itemsets = []    \n",
    "    for each in res:\n",
    "        full_ratings_predicted = list(each)\n",
    "        full_ratings_indexed = list(enumerate(full_ratings_predicted))\n",
    "        final_itemsets.append(sorted(full_ratings_indexed,key=lambda x:x[1],reverse =True)[:k])\n",
    "        \n",
    "    return final_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = make_top_k_recommendations(autoencoder_network, evidence, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_movies = [[r[0] for r in recommendation] for recommendation in recommendations]\n",
    "movies_ratings = [[r[1] for r in recommendation] for recommendation in recommendations]\n",
    "\n",
    "user_recommendation = pd.DataFrame(recommendation_movies)\n",
    "user_recommendation = user_recommendation.applymap(lambda x:id_2_movie[x])\n",
    "\n",
    "recommendation_ratings = pd.DataFrame(movies_ratings)\n",
    "\n",
    "user_recommendation.to_csv(os.path.join(OUTPUT_PATH, OUTPUT_FILE))\n",
    "recommendation_ratings.to_csv(os.path.join(OUTPUT_PATH, OUTPUT_R_FILE))\n",
    "print(f\"Successfully saved user recommendations to file: {os.path.join(OUTPUT_PATH, OUTPUT_FILE)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWM",
   "language": "python",
   "name": "swm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
