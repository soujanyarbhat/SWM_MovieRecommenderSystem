{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as weight_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for Symmetrical AutoEncoder Network\n",
    "    :param L: List of int, contains sizes of encoding layers and starts with feature size\n",
    "    For example: [500, 20, 10] will result in:\n",
    "      - encoder 2 layers: 500x20 and 20x10. Representation layer (z) will be 10\n",
    "      - decoder 2 layers: 10x20 and 20x500, output size is 500, reconstructed.\n",
    "    :param activation_fn: (default 'sigmoid') Type of activation function\n",
    "    :param drop_prob: (default: 0.0) Dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, L, activation_fn='sigmoid', drop_prob=0.0):\n",
    "        super(Encoder, self).__init__()\n",
    "        layers = self.create_nn_structure(L)\n",
    "        self.num_layers = len(L)\n",
    "        # create controller for activation function\n",
    "        self.activation_fn_nm = activation_fn\n",
    "        # create dropout module\n",
    "        self._drop_prob = drop_prob\n",
    "        if drop_prob > 0.0:\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "        #initialize with empty list to store layers\n",
    "        self.linears = nn.ModuleList([])\n",
    "        self.linears.extend([nn.Linear(i[0], i[1]) for i in layers])\n",
    "        \n",
    "    def get_activation_fn(self):\n",
    "        # user selected activation function at layers except for last layer\n",
    "        if self.activation_fn_nm == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif self.activation_fn_nm == 'lrelu':\n",
    "            return nn.LeakyReLU()\n",
    "        elif self.activation_fn_nm == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError('Activation function type not defined')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i,layer in enumerate(self.linears):\n",
    "            if i <= self.num_layers-1:\n",
    "                # create instance of activation function\n",
    "                act_fn = self.get_activation_fn()\n",
    "                # pass in the input\n",
    "                x = act_fn(self.linears[i](x))\n",
    "                if self._drop_prob > 0.0 and i <= int(self.num_layers/2): \n",
    "                    # apply dropout only on encode layer by control of i\n",
    "                    x = self.dropout(x)\n",
    "        # No activation on the last decoding layer\n",
    "        x = self.linears[-1](x)\n",
    "        return x\n",
    "\n",
    "    def create_nn_structure(self, L):\n",
    "        max_ind = len(L)-1\n",
    "        layers = []\n",
    "        for i,v in enumerate(L):\n",
    "            if i < max_ind:\n",
    "                #still have i+1 available, create layer tuple\n",
    "                layer = [v,L[i+1]]\n",
    "                layers.append(layer)\n",
    "        #then inverse the layers for decoder size\n",
    "        encoder_layers = layers[:]\n",
    "        for l in encoder_layers[::-1]:\n",
    "            decoder_layer = l[::-1]\n",
    "            layers.append(decoder_layer)\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_mapping(L):\n",
    "    '''\n",
    "    return reindexed dict on user and items\n",
    "    encoded indices starts from 1\n",
    "    input: \n",
    "    * L: list of str\n",
    "    outputs:\n",
    "    * ind_2_item,item_2_ind: tuple of dictionary\n",
    "    '''\n",
    "    L = set(L)\n",
    "    ind_2_item = {}\n",
    "    \n",
    "    for i,v in enumerate(L):\n",
    "        #index start from 1\n",
    "        ind_2_item[i+1] = v\n",
    "    #invert the map\n",
    "    item_2_ind = {v: k for k, v in ind_2_item.items()}\n",
    "    return ind_2_item,item_2_ind\n",
    "    \n",
    "def reindexer(ratings_df,user_col,item_col,rating_col):\n",
    "    '''\n",
    "    inputs:\n",
    "    * ratings_df: pandas df containing ratings/affinity for user-item pairs\n",
    "    * user_col: actual col name for users\n",
    "    * item_col: actual col name for items\n",
    "    * rating_col: actual col name for ratings\n",
    "    output:\n",
    "    * ratings_df: reindexed user and item column, pandas df\n",
    "    '''\n",
    "    users_list = ratings_df[user_col].tolist()\n",
    "    item_list = ratings_df[item_col].tolist()\n",
    "    \n",
    "    ind_2_user,user_2_ind = create_index_mapping(users_list)\n",
    "    ind_2_item,item_2_ind = create_index_mapping(item_list)\n",
    "    \n",
    "    #rename ratings df\n",
    "    ratings_df = ratings_df.rename(columns={user_col:'user_col',\n",
    "                                            item_col:'item_col',\n",
    "                                            rating_col:'rating_col'})\n",
    "\n",
    "    #encode df using the 2 mappings\n",
    "    ratings_df['encoded_users'] = ratings_df['user_col'].apply(lambda x:user_2_ind[x])\n",
    "    ratings_df['encoded_items'] = ratings_df['item_col'].apply(lambda x:item_2_ind[x])\n",
    "    \n",
    "    return ratings_df[['encoded_users','encoded_items','rating_col']]\n",
    "  \n",
    "ratings = pd.read_csv('MovieLens1M/ratings.dat', delimiter = '::',header=None, engine='python')\n",
    "ratings.columns = ['userid','movieid','rating','timestamp']\n",
    "ratings_reindex = reindexer(ratings,'userid','movieid','rating')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(ratings_reindex,\n",
    "                               stratify=ratings_reindex['encoded_users'],\n",
    "                               test_size=0.1,\n",
    "                               random_state=42)\n",
    "\n",
    "training_set = np.array(train, dtype = 'int')\n",
    "test_set = np.array(test, dtype = 'int')\n",
    "\n",
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))\n",
    "\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(nb_users+1):\n",
    "        # each user's watched movies\n",
    "        # data[:,0], first column, all rows column users\n",
    "        id_items = data[:,1][data[:,0] == id_users]\n",
    "        # each user's rating for that item\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        # the positions of these items are filled with ratings, creating the matrix\n",
    "        ratings[id_items-1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "  \n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)\n",
    "\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: tensor(1.2801)\n",
      "epoch: 2 loss: tensor(0.9990)\n",
      "epoch: 3 loss: tensor(0.9853)\n",
      "epoch: 4 loss: tensor(0.9805)\n",
      "epoch: 5 loss: tensor(0.9783)\n",
      "epoch: 6 loss: tensor(0.9769)\n",
      "epoch: 7 loss: tensor(0.9765)\n",
      "epoch: 8 loss: tensor(0.9757)\n",
      "epoch: 9 loss: tensor(0.9756)\n",
      "epoch: 10 loss: tensor(0.9749)\n",
      "test loss: tensor(0.9524)\n"
     ]
    }
   ],
   "source": [
    "autoencoder_network = Encoder([nb_movies,20,10],'sigmoid',0.1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(autoencoder_network.parameters(), lr = 0.01, weight_decay = 0.5)\n",
    "\n",
    "nb_epoch = 10\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    # s is the number of users who rated at least 1 movies\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = autoencoder_network(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) #making this anyway not equal to 0, as this will be a denominator\n",
    "            #mean_corrector is the avg of the error, only considering the movies having ratings (non-zero ratings) for computing mean of error\n",
    "            loss.backward() # decide the direction the increment of weights\n",
    "            #this call will just computing all the gradients required\n",
    "            train_loss += np.sqrt(loss.data*mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step() # decide the amount to update the weights\n",
    "            \n",
    "    print('epoch: '+str(epoch)+' loss: '+ str(train_loss/s))\n",
    "    \n",
    "    \n",
    "test_loss = 0\n",
    "s = 0.\n",
    "\n",
    "res = []\n",
    "targets = []\n",
    "\n",
    "# averaged difference between real rating and predicted rating\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0) # should keep the training set\n",
    "    target = Variable(test_set[id_user]).unsqueeze(0) # to predict the other movies user not seen yet\n",
    "    \n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        # make predictions\n",
    "        output = autoencoder_network(input)\n",
    "        targets.append(target.detach().numpy())\n",
    "        res.append(output.detach().numpy()) \n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0 # dont want to measure the loss on the movies didnt get the actual rating from user \n",
    "        # force to 0 and difference / loss will be 0 for those entries\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) \n",
    "        # only consider the movies that are rated in the test set, to be included in the loss\n",
    "        test_loss += np.sqrt(loss.data*mean_corrector)\n",
    "        s += 1.\n",
    "print('test loss: '+str(test_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making top k recommendation\n",
    "def make_top_k_recommendations(encoder,evidence,k,filter_seen=True):\n",
    "    '''\n",
    "    :param encoder: autoencoder instance\n",
    "    :param evidence: full set of seen ratings from all users\n",
    "    :param k: top k items (by output score)\n",
    "    :param filter_seen: (default True) filter controller to remove seen items from top k list\n",
    "    '''     \n",
    "    res = []\n",
    "    nb_users = evidence.shape[0]\n",
    "    # to find top scored items for each user\n",
    "    for id_user in range(nb_users):\n",
    "        encoder_input = Variable(evidence[id_user]).unsqueeze(0) # should keep the training set \n",
    "        encoder_output = encoder(encoder_input)\n",
    "        \n",
    "        target = Variable(evidence[id_user]).unsqueeze(0) # mask to find items not seen yet\n",
    "        if filter_seen:\n",
    "            encoder_output[target != 0] = 0 # force seen items scores to 0, will never get recommended\n",
    "        res.append(encoder_output.detach().numpy())\n",
    "        \n",
    "    res = [a[0] for a in res]\n",
    "    final_itemsets = []    \n",
    "    for each in res:\n",
    "        full_ratings_predicted = list(each)\n",
    "        full_ratings_indexed = list(enumerate(full_ratings_predicted))\n",
    "        final_itemsets.append(sorted(full_ratings_indexed,key=lambda x:x[1],reverse =True)[:k])\n",
    "        \n",
    "    return final_itemsets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWM",
   "language": "python",
   "name": "swm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
