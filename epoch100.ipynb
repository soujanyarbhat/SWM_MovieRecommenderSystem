{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNcsipx7+g+W/N4Y8aoOx+I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soujanyarbhat/SWM_MovieRecommenderSystem/blob/main/epoch100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COzxFl2_3uIo"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.init as weight_init\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9tlYFMv3y6G"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Class for Symmetrical AutoEncoder Network\n",
        "    :param L: List of int, contains sizes of encoding layers and starts with feature size\n",
        "    For example: [500, 20, 10] will result in:\n",
        "      - encoder 2 layers: 500x20 and 20x10. Representation layer (z) will be 10\n",
        "      - decoder 2 layers: 10x20 and 20x500, output size is 500, reconstructed.\n",
        "    :param activation_fn: (default 'sigmoid') Type of activation function\n",
        "    :param drop_prob: (default: 0.0) Dropout probability\n",
        "    \"\"\"\n",
        "    def __init__(self, L, activation_fn='sigmoid', drop_prob=0.0):\n",
        "        super(Encoder, self).__init__()\n",
        "        layers = self.create_nn_structure(L)\n",
        "        self.num_layers = len(L)\n",
        "        # create controller for activation function\n",
        "        self.activation_fn_nm = activation_fn\n",
        "        # create dropout module\n",
        "        self._drop_prob = drop_prob\n",
        "        if drop_prob > 0.0:\n",
        "            self.dropout = nn.Dropout(drop_prob)\n",
        "        #initialize with empty list to store layers\n",
        "        self.linears = nn.ModuleList([])\n",
        "        self.linears.extend([nn.Linear(i[0], i[1]) for i in layers])\n",
        "        \n",
        "    def get_activation_fn(self):\n",
        "        # user selected activation function at layers except for last layer\n",
        "        if self.activation_fn_nm == 'relu':\n",
        "            return nn.ReLU()\n",
        "        elif self.activation_fn_nm == 'lrelu':\n",
        "            return nn.LeakyReLU()\n",
        "        elif self.activation_fn_nm == 'sigmoid':\n",
        "            return nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError('Activation function type not defined')\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for i,layer in enumerate(self.linears):\n",
        "            if i <= self.num_layers-1:\n",
        "                # create instance of activation function\n",
        "                act_fn = self.get_activation_fn()\n",
        "                # pass in the input\n",
        "                x = act_fn(self.linears[i](x))\n",
        "                if self._drop_prob > 0.0 and i <= int(self.num_layers/2): \n",
        "                    # apply dropout only on encode layer by control of i\n",
        "                    x = self.dropout(x)\n",
        "        # No activation on the last decoding layer\n",
        "        x = self.linears[-1](x)\n",
        "        return x\n",
        "\n",
        "    def create_nn_structure(self, L):\n",
        "        max_ind = len(L)-1\n",
        "        layers = []\n",
        "        for i,v in enumerate(L):\n",
        "            if i < max_ind:\n",
        "                #still have i+1 available, create layer tuple\n",
        "                layer = [v,L[i+1]]\n",
        "                layers.append(layer)\n",
        "        #then inverse the layers for decoder size\n",
        "        encoder_layers = layers[:]\n",
        "        for l in encoder_layers[::-1]:\n",
        "            decoder_layer = l[::-1]\n",
        "            layers.append(decoder_layer)\n",
        "        return layers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FUQaa5S333I",
        "outputId": "38b9a983-4ba0-418d-8e7a-cac2ecb3b941"
      },
      "source": [
        "def create_index_mapping(L):\n",
        "    '''\n",
        "    return reindexed dict on user and items\n",
        "    encoded indices starts from 1\n",
        "    input: \n",
        "    * L: list of str\n",
        "    outputs:\n",
        "    * ind_2_item,item_2_ind: tuple of dictionary\n",
        "    '''\n",
        "    L = set(L)\n",
        "    ind_2_item = {}\n",
        "    \n",
        "    for i,v in enumerate(L):\n",
        "        #index start from 1\n",
        "        ind_2_item[i+1] = v\n",
        "    #invert the map\n",
        "    item_2_ind = {v: k for k, v in ind_2_item.items()}\n",
        "    return ind_2_item,item_2_ind\n",
        "    \n",
        "def reindexer(ratings_df,user_col,item_col,rating_col):\n",
        "    '''\n",
        "    inputs:\n",
        "    * ratings_df: pandas df containing ratings/affinity for user-item pairs\n",
        "    * user_col: actual col name for users\n",
        "    * item_col: actual col name for items\n",
        "    * rating_col: actual col name for ratings\n",
        "    output:\n",
        "    * ratings_df: reindexed user and item column, pandas df\n",
        "    '''\n",
        "    users_list = ratings_df[user_col].tolist()\n",
        "    item_list = ratings_df[item_col].tolist()\n",
        "    \n",
        "    ind_2_user,user_2_ind = create_index_mapping(users_list)\n",
        "    ind_2_item,item_2_ind = create_index_mapping(item_list)\n",
        "    \n",
        "    #rename ratings df\n",
        "    ratings_df = ratings_df.rename(columns={user_col:'user_col',\n",
        "                                            item_col:'item_col',\n",
        "                                            rating_col:'rating_col'})\n",
        "\n",
        "    #encode df using the 2 mappings\n",
        "    ratings_df['encoded_users'] = ratings_df['user_col'].apply(lambda x:user_2_ind[x])\n",
        "    ratings_df['encoded_items'] = ratings_df['item_col'].apply(lambda x:item_2_ind[x])\n",
        "    \n",
        "    return ratings_df[['encoded_users','encoded_items','rating_col']]\n",
        "  \n",
        "ratings = pd.read_csv('/content/ratings.dat', delimiter = '::',header=None, engine='python')\n",
        "ratings.columns = ['userid','movieid','rating','timestamp']\n",
        "ratings_reindex = reindexer(ratings,'userid','movieid','rating')\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(ratings_reindex,\n",
        "                               stratify=ratings_reindex['encoded_users'],\n",
        "                               test_size=0.1,\n",
        "                               random_state=42)\n",
        "\n",
        "training_set = np.array(train, dtype = 'int')\n",
        "test_set = np.array(test, dtype = 'int')\n",
        "\n",
        "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
        "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))\n",
        "\n",
        "def convert(data):\n",
        "    new_data = []\n",
        "    for id_users in range(nb_users+1):\n",
        "        # each user's watched movies\n",
        "        # data[:,0], first column, all rows column users\n",
        "        id_items = data[:,1][data[:,0] == id_users]\n",
        "        # each user's rating for that item\n",
        "        id_ratings = data[:,2][data[:,0] == id_users]\n",
        "        ratings = np.zeros(nb_movies)\n",
        "        # the positions of these items are filled with ratings, creating the matrix\n",
        "        ratings[id_items-1] = id_ratings\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data\n",
        "  \n",
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)\n",
        "\n",
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)\n",
        "print(training_set.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6041, 3706])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZbwTIse4Nxm",
        "outputId": "7a826e62-11e1-49d8-a114-fc96de4c5ac7"
      },
      "source": [
        "autoencoder_network = Encoder([nb_movies,20,10],'sigmoid',0.1)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(autoencoder_network.parameters(), lr = 0.01, weight_decay = 0.5)\n",
        "\n",
        "nb_epoch = 100\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "    train_loss = 0\n",
        "    s = 0.\n",
        "    # s is the number of users who rated at least 1 movies\n",
        "    for id_user in range(nb_users):\n",
        "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "        target = input.clone()\n",
        "        if torch.sum(target.data > 0) > 0:\n",
        "            output = autoencoder_network(input)\n",
        "            target.require_grad = False\n",
        "            output[target == 0] = 0\n",
        "            loss = criterion(output, target)\n",
        "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) #making this anyway not equal to 0, as this will be a denominator\n",
        "            #mean_corrector is the avg of the error, only considering the movies having ratings (non-zero ratings) for computing mean of error\n",
        "            loss.backward() # decide the direction the increment of weights\n",
        "            #this call will just computing all the gradients required\n",
        "            train_loss += np.sqrt(loss.data*mean_corrector)\n",
        "            s += 1.\n",
        "            optimizer.step() # decide the amount to update the weights\n",
        "            \n",
        "    print('epoch: '+str(epoch)+' loss: '+ str(train_loss/s))\n",
        "    \n",
        "    \n",
        "test_loss = 0\n",
        "s = 0.\n",
        "\n",
        "res = []\n",
        "targets = []\n",
        "\n",
        "# averaged difference between real rating and predicted rating\n",
        "for id_user in range(nb_users):\n",
        "    input = Variable(training_set[id_user]).unsqueeze(0) # should keep the training set\n",
        "    target = Variable(test_set[id_user]).unsqueeze(0) # to predict the other movies user not seen yet\n",
        "    \n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "        # make predictions\n",
        "        output = autoencoder_network(input)\n",
        "        targets.append(target.detach().numpy())\n",
        "        res.append(output.detach().numpy()) \n",
        "        target.require_grad = False\n",
        "        output[target == 0] = 0 # dont want to measure the loss on the movies didnt get the actual rating from user \n",
        "        # force to 0 and difference / loss will be 0 for those entries\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) \n",
        "        # only consider the movies that are rated in the test set, to be included in the loss\n",
        "        test_loss += np.sqrt(loss.data*mean_corrector)\n",
        "        s += 1.\n",
        "print('test loss: '+str(test_loss/s))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 loss: tensor(1.2800)\n",
            "epoch: 2 loss: tensor(0.9991)\n",
            "epoch: 3 loss: tensor(0.9852)\n",
            "epoch: 4 loss: tensor(0.9804)\n",
            "epoch: 5 loss: tensor(0.9784)\n",
            "epoch: 6 loss: tensor(0.9771)\n",
            "epoch: 7 loss: tensor(0.9766)\n",
            "epoch: 8 loss: tensor(0.9758)\n",
            "epoch: 9 loss: tensor(0.9756)\n",
            "epoch: 10 loss: tensor(0.9750)\n",
            "epoch: 11 loss: tensor(0.9751)\n",
            "epoch: 12 loss: tensor(0.9746)\n",
            "epoch: 13 loss: tensor(0.9747)\n",
            "epoch: 14 loss: tensor(0.9743)\n",
            "epoch: 15 loss: tensor(0.9744)\n",
            "epoch: 16 loss: tensor(0.9742)\n",
            "epoch: 17 loss: tensor(0.9744)\n",
            "epoch: 18 loss: tensor(0.9740)\n",
            "epoch: 19 loss: tensor(0.9742)\n",
            "epoch: 20 loss: tensor(0.9738)\n",
            "epoch: 21 loss: tensor(0.9741)\n",
            "epoch: 22 loss: tensor(0.9739)\n",
            "epoch: 23 loss: tensor(0.9739)\n",
            "epoch: 24 loss: tensor(0.9736)\n",
            "epoch: 25 loss: tensor(0.9735)\n",
            "epoch: 26 loss: tensor(0.9734)\n",
            "epoch: 27 loss: tensor(0.9734)\n",
            "epoch: 28 loss: tensor(0.9731)\n",
            "epoch: 29 loss: tensor(0.9728)\n",
            "epoch: 30 loss: tensor(0.9718)\n",
            "epoch: 31 loss: tensor(0.9717)\n",
            "epoch: 32 loss: tensor(0.9708)\n",
            "epoch: 33 loss: tensor(0.9701)\n",
            "epoch: 34 loss: tensor(0.9699)\n",
            "epoch: 35 loss: tensor(0.9698)\n",
            "epoch: 36 loss: tensor(0.9700)\n",
            "epoch: 37 loss: tensor(0.9692)\n",
            "epoch: 38 loss: tensor(0.9693)\n",
            "epoch: 39 loss: tensor(0.9703)\n",
            "epoch: 40 loss: tensor(0.9682)\n",
            "epoch: 41 loss: tensor(0.9694)\n",
            "epoch: 42 loss: tensor(0.9702)\n",
            "epoch: 43 loss: tensor(0.9677)\n",
            "epoch: 44 loss: tensor(0.9683)\n",
            "epoch: 45 loss: tensor(0.9680)\n",
            "epoch: 46 loss: tensor(0.9689)\n",
            "epoch: 47 loss: tensor(0.9664)\n",
            "epoch: 48 loss: tensor(0.9666)\n",
            "epoch: 49 loss: tensor(0.9656)\n",
            "epoch: 50 loss: tensor(0.9645)\n",
            "epoch: 51 loss: tensor(0.9646)\n",
            "epoch: 52 loss: tensor(0.9634)\n",
            "epoch: 53 loss: tensor(0.9621)\n",
            "epoch: 54 loss: tensor(0.9612)\n",
            "epoch: 55 loss: tensor(0.9609)\n",
            "epoch: 56 loss: tensor(0.9588)\n",
            "epoch: 57 loss: tensor(0.9628)\n",
            "epoch: 58 loss: tensor(0.9620)\n",
            "epoch: 59 loss: tensor(0.9607)\n",
            "epoch: 60 loss: tensor(0.9602)\n",
            "epoch: 61 loss: tensor(0.9594)\n",
            "epoch: 62 loss: tensor(0.9600)\n",
            "epoch: 63 loss: tensor(0.9601)\n",
            "epoch: 64 loss: tensor(0.9579)\n",
            "epoch: 65 loss: tensor(0.9632)\n",
            "epoch: 66 loss: tensor(0.9601)\n",
            "epoch: 67 loss: tensor(0.9590)\n",
            "epoch: 68 loss: tensor(0.9585)\n",
            "epoch: 69 loss: tensor(0.9584)\n",
            "epoch: 70 loss: tensor(0.9576)\n",
            "epoch: 71 loss: tensor(0.9561)\n",
            "epoch: 72 loss: tensor(0.9557)\n",
            "epoch: 73 loss: tensor(0.9535)\n",
            "epoch: 74 loss: tensor(0.9534)\n",
            "epoch: 75 loss: tensor(0.9561)\n",
            "epoch: 76 loss: tensor(0.9534)\n",
            "epoch: 77 loss: tensor(0.9530)\n",
            "epoch: 78 loss: tensor(0.9544)\n",
            "epoch: 79 loss: tensor(0.9564)\n",
            "epoch: 80 loss: tensor(0.9531)\n",
            "epoch: 81 loss: tensor(0.9543)\n",
            "epoch: 82 loss: tensor(0.9523)\n",
            "epoch: 83 loss: tensor(0.9539)\n",
            "epoch: 84 loss: tensor(0.9514)\n",
            "epoch: 85 loss: tensor(0.9519)\n",
            "epoch: 86 loss: tensor(0.9487)\n",
            "epoch: 87 loss: tensor(0.9500)\n",
            "epoch: 88 loss: tensor(0.9485)\n",
            "epoch: 89 loss: tensor(0.9493)\n",
            "epoch: 90 loss: tensor(0.9498)\n",
            "epoch: 91 loss: tensor(0.9538)\n",
            "epoch: 92 loss: tensor(0.9548)\n",
            "epoch: 93 loss: tensor(0.9524)\n",
            "epoch: 94 loss: tensor(0.9477)\n",
            "epoch: 95 loss: tensor(0.9463)\n",
            "epoch: 96 loss: tensor(0.9487)\n",
            "epoch: 97 loss: tensor(0.9470)\n",
            "epoch: 98 loss: tensor(0.9461)\n",
            "epoch: 99 loss: tensor(0.9458)\n",
            "epoch: 100 loss: tensor(0.9459)\n",
            "test loss: tensor(0.9240)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}